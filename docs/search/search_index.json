{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Home Welcome to the knowledge pool for the Danish EuroHPC competence centre . Here you will find best practices, material about HPC, HPDA and AI software and hardware, Analysis of the Danish national HPCs infrastructure, relevant resources and literature. All the material can be found in pdf form for offline consultation by clicking on the symbol on the top-right corner of this page. This knowledge pool is meant to be available both to completely new users and advanced/expert users. If you are a new user, visit the page HPC 101 to learn the basics about High Performance Computing and get up to speed. Facilitation in accessing HPCs and dissemination activities are offered - if any is needed, feel free to contact eurocc@listserv.deic.dk . If you desire a specific topic or material to be covered, or need any type of assistance related with the topics of the knowledge pool, contact the HPC facilitator at eurocc@listserv.deic.dk .","title":"Home"},{"location":"#home","text":"Welcome to the knowledge pool for the Danish EuroHPC competence centre . Here you will find best practices, material about HPC, HPDA and AI software and hardware, Analysis of the Danish national HPCs infrastructure, relevant resources and literature. All the material can be found in pdf form for offline consultation by clicking on the symbol on the top-right corner of this page. This knowledge pool is meant to be available both to completely new users and advanced/expert users. If you are a new user, visit the page HPC 101 to learn the basics about High Performance Computing and get up to speed. Facilitation in accessing HPCs and dissemination activities are offered - if any is needed, feel free to contact eurocc@listserv.deic.dk . If you desire a specific topic or material to be covered, or need any type of assistance related with the topics of the knowledge pool, contact the HPC facilitator at eurocc@listserv.deic.dk .","title":"Home"},{"location":"best/","text":"Best practices for HPC This page lists some useful best practices to keep in mind when coding and running applications and pipelines on HPC systems. Code coverage, testing, continuous integration Every time we code, testing is a concern and is usually performed by the coder(s) regularly during the project. One can identify some basic main types of test: Regression test Given an expected output from a specific input, the code is tested to reproduce that same output. Unit test Tests the smallest units of the software (e.g. single functions) to identify bugs, especially in extreme cases of inputs and outputs Continuous integration A set of tests the software runs automatically everytime the code is updated. This is useful to spot bugs before someone even uses the code. More things one might need to test are the performance/scalability of the code, usability, and response to all the intended types of input data. Unit and regression test can be useful, but at some point not really feasible, since the code can scale to be quite large and complex, with a lot of things to control. It is thus a good practice to use continuous integration, and implement simple but representative tests that cover all the code, so that bugs can be spotted often before the final users do that. Code coverage tools to implement such tests exists for several programming languages, and also for testing code deployed on GitHub version control. Link Description pyTest A package to test python code Cmake To test both C , C++ and Fortran code Travis CI Tool for continuous integration in most of the used programming languages. Works on Git version control. covr Test coverage reports for R Code styling An important feature of a computer code is that it is understandable to other people reading it. To ensure this is the case, a clean and coherent style of coding should be used in a project. Some languages have a preferred coding style, and in some GUIs (graphical user interfaces) those styling rules can be set to be required. One can also use ones own coding style, but it should be one easily readable by others, and it should be the same style throughout the whole project. Link Description styleguide Google guide for coding styles of the major programming languages awesome guidelines A guide to coding styles covering also documentations, tools and development environments Pythonic rules Intoduction to coding style in python. R style A post on R coding style Containerized applications In this section the benefits of project and package managers, that are a way of organizing packages in separated environments, will be outlined. However, a higher degree of isolation can be achieved by containerization than using environments. By containerizing, a user can virtualize the entire operating system, and make it ready to be deployed on any other machine. One can for example deploy a container without the need of installing anything on the hosting machine! Note that containers are a different concept from Virtual Machines, where it is the hardware being virtualized instead. Link Description Docker An open source widespread container that is popular both in research and industry Docker course A course on the use of Docker freely hosted on youtube Docker curriculum Beginner's introduction to docker Docker basics Intoduction tutorials to Docker from the official documentation page Singularity Singularity is another containerization tool. It allows you to decide at which degree a container interacts with the hosting system Singularity tutorial A well done Singularity tutorial for HPC users Singularity video tutorial A video tutorial on Singularity Reproducibility by containerization A video on reproducibility with Singularity containers Documentation When creating a piece of software, it is always a good idea to create a documentation explaining the usage of each element of the code. For packages, there are software that automatically create a documentation by using the declarations of functions and eventually some text included into them as a string. Link Description MkDocs A generator for static webpages, with design and themes targeted to documentation pages, but also other type of websites. This website is itself made with MkDocs. mkdocstrings Python handler to automatically generate documentation with MkDocs pdoc3 A package that automatically creates the documentation for your coding projects. It is semi-automatic (infers your dependencies, classes, etc. but adds a description based on your docstrings) pdoc3 101 How to run pdoc to create an HTML documentation Roxygen2 A package to generate R documentation \u2014 it can be used also with Rcpp Sphinx Another tool to write documentation \u2014 it produces also printable outputs. Sphinx was first created to write the python language documentation. Even though it is a tool especially thought for python code, it can be used to generate static webpages for other projects. Documents with live code Programming languages like python and R allows users to write documents that contain text, images and equations together with executable code and its output. Text is usually written using the very immediate markdown language . Markdown files for R can be created in the GUI Rstudio , while python uses jupyter notebooks . Link Description Introduction to Markdown Markdown for R in Rstudio Jupyter notebooks create interactive code with python . You can write R code in a jupyter notebook by using the python package rpy2 Package/Environment management systems When coding, it is essential that all the projects are developed under specific software conditions, i.e. the packages and libraries used during development (dependencies) should not change along the project's lifetime, so that variations in things such as output formats and new algorithmic implementations will not create conflicts difficult to trace back under development. An environment and package manager makes the user able to create separated frameworks (environments) where to install specific packages that will not influence other software outside the environment in use. A higher degree of isolation can be achieved through containers (see the related part of this page). Link Description Conda an easy to use and very popular environment manager Getting started with conda Introduction to conda setup and usage from the official documentation Conda cheat sheet Quick reference for conda usage YARN An alternative to conda Many short jobs running Every time a job is submitted to the job manager (e.g. Slurm) of a computing cluster, there is an overhead time necessary to elaborate resource provision, preparation for output, and queue organization. Therefore it is wise to create, when possible, longer jobs. One needs to find the correct balance for how to organizing jobs: if these are too long and fail because of some issue, than a lot of time and resources have been wasted, but such problems can be overcome by tracking the outputs of each step to avoid rerunning all computations. For example, at each step of a job outputting something relevant, there can be a condition checking if the specific output is already present. Massive standard outputs Try to avoid printing many outputs on the standard output ( stdout ), in other words a large amount of printed outputs directly to the terminal screen. This can be problematic when a lot of parallel jobs are running, letting stdout filling all the home directory up, and causing errors and eventual data loss. Instead use an output in software-specific data structures (such as .RData files for the R language) or at least simple text files. Packaging a coding project When coding a piece of software in which there are multiple newly implemented functions, it can be smart to organize all those functions as a package, that can be reused and eventually shared with ease. Such a practice is especially easy and can be mastered very quickly for coding projects in python and R . Link Description pyPA python packaging user guide R package development Develop an R package using Rstudio Pipe-lining and submitting jobs in Slurm Slurm is a job scheduler. It allows a user to specify a series of commands and resources requirements to run such commands. Slurm does consider the job submission on an HPC system together with all the other jobs, and prioritize them among other things according to the resources requirement and the available computational power. In figure above, the priority assigned to a Slurm job when the requested time increases, by keeping the memory and CPUs fixed. Decreased priority has higher values. Adapted from A Slurm Simulator: Implementation and Parametric Analysis. Simakov et al 2017. The Danish national HPCs, and most of the other EuroHPC supercomputers, use Slurm as job manager. Link Description Interactive Slurm An interactive Slurm tutorial, in which you will be guided through the process of using Slurm. SLURM example 1 and SLURM example 2 Some examples of how to make a Slurm script to submit a job from the danish HPC GenomeDK and from Princeton Research Computing. Gwf, a simple python tool to create interdependent job submissions Gwf, developed at the University of Aarhus, makes it easy to create Slurm jobs and organize them as a pipeline with dependencies, using the python language (you need python 3.5+). You get to simply create the shell scripts and the dependencies, without the complicating syntax of Slurm. The page contains also a useful guide. Version control Version control is the tracking of your development history for a project. This allows multiple people working on the same material to keep changes in sync without stepping over each other's contributions. Version control tools allow to commit changes with a description, set up and assign project objectives, open software issues from users and contributors, test automatically the code to find bugs before users step into them. Version control is useful for both teams and single users, and it is a good practice to have version control as a standard for any project. Link Description GitHub the most used tool for version control Github 101 quick introduction to get started on Github GitLab and BitBucket Two other popular alternatives to Github","title":"Best practices"},{"location":"best/#best-practices-for-hpc","text":"This page lists some useful best practices to keep in mind when coding and running applications and pipelines on HPC systems.","title":"Best practices for HPC"},{"location":"best/#code-coverage-testing-continuous-integration","text":"Every time we code, testing is a concern and is usually performed by the coder(s) regularly during the project. One can identify some basic main types of test: Regression test Given an expected output from a specific input, the code is tested to reproduce that same output. Unit test Tests the smallest units of the software (e.g. single functions) to identify bugs, especially in extreme cases of inputs and outputs Continuous integration A set of tests the software runs automatically everytime the code is updated. This is useful to spot bugs before someone even uses the code. More things one might need to test are the performance/scalability of the code, usability, and response to all the intended types of input data. Unit and regression test can be useful, but at some point not really feasible, since the code can scale to be quite large and complex, with a lot of things to control. It is thus a good practice to use continuous integration, and implement simple but representative tests that cover all the code, so that bugs can be spotted often before the final users do that. Code coverage tools to implement such tests exists for several programming languages, and also for testing code deployed on GitHub version control. Link Description pyTest A package to test python code Cmake To test both C , C++ and Fortran code Travis CI Tool for continuous integration in most of the used programming languages. Works on Git version control. covr Test coverage reports for R","title":"Code coverage, testing, continuous integration"},{"location":"best/#code-styling","text":"An important feature of a computer code is that it is understandable to other people reading it. To ensure this is the case, a clean and coherent style of coding should be used in a project. Some languages have a preferred coding style, and in some GUIs (graphical user interfaces) those styling rules can be set to be required. One can also use ones own coding style, but it should be one easily readable by others, and it should be the same style throughout the whole project. Link Description styleguide Google guide for coding styles of the major programming languages awesome guidelines A guide to coding styles covering also documentations, tools and development environments Pythonic rules Intoduction to coding style in python. R style A post on R coding style","title":"Code styling"},{"location":"best/#containerized-applications","text":"In this section the benefits of project and package managers, that are a way of organizing packages in separated environments, will be outlined. However, a higher degree of isolation can be achieved by containerization than using environments. By containerizing, a user can virtualize the entire operating system, and make it ready to be deployed on any other machine. One can for example deploy a container without the need of installing anything on the hosting machine! Note that containers are a different concept from Virtual Machines, where it is the hardware being virtualized instead. Link Description Docker An open source widespread container that is popular both in research and industry Docker course A course on the use of Docker freely hosted on youtube Docker curriculum Beginner's introduction to docker Docker basics Intoduction tutorials to Docker from the official documentation page Singularity Singularity is another containerization tool. It allows you to decide at which degree a container interacts with the hosting system Singularity tutorial A well done Singularity tutorial for HPC users Singularity video tutorial A video tutorial on Singularity Reproducibility by containerization A video on reproducibility with Singularity containers","title":"Containerized applications"},{"location":"best/#documentation","text":"When creating a piece of software, it is always a good idea to create a documentation explaining the usage of each element of the code. For packages, there are software that automatically create a documentation by using the declarations of functions and eventually some text included into them as a string. Link Description MkDocs A generator for static webpages, with design and themes targeted to documentation pages, but also other type of websites. This website is itself made with MkDocs. mkdocstrings Python handler to automatically generate documentation with MkDocs pdoc3 A package that automatically creates the documentation for your coding projects. It is semi-automatic (infers your dependencies, classes, etc. but adds a description based on your docstrings) pdoc3 101 How to run pdoc to create an HTML documentation Roxygen2 A package to generate R documentation \u2014 it can be used also with Rcpp Sphinx Another tool to write documentation \u2014 it produces also printable outputs. Sphinx was first created to write the python language documentation. Even though it is a tool especially thought for python code, it can be used to generate static webpages for other projects.","title":"Documentation"},{"location":"best/#documents-with-live-code","text":"Programming languages like python and R allows users to write documents that contain text, images and equations together with executable code and its output. Text is usually written using the very immediate markdown language . Markdown files for R can be created in the GUI Rstudio , while python uses jupyter notebooks . Link Description Introduction to Markdown Markdown for R in Rstudio Jupyter notebooks create interactive code with python . You can write R code in a jupyter notebook by using the python package rpy2","title":"Documents with live code"},{"location":"best/#packageenvironment-management-systems","text":"When coding, it is essential that all the projects are developed under specific software conditions, i.e. the packages and libraries used during development (dependencies) should not change along the project's lifetime, so that variations in things such as output formats and new algorithmic implementations will not create conflicts difficult to trace back under development. An environment and package manager makes the user able to create separated frameworks (environments) where to install specific packages that will not influence other software outside the environment in use. A higher degree of isolation can be achieved through containers (see the related part of this page). Link Description Conda an easy to use and very popular environment manager Getting started with conda Introduction to conda setup and usage from the official documentation Conda cheat sheet Quick reference for conda usage YARN An alternative to conda","title":"Package/Environment management systems"},{"location":"best/#many-short-jobs-running","text":"Every time a job is submitted to the job manager (e.g. Slurm) of a computing cluster, there is an overhead time necessary to elaborate resource provision, preparation for output, and queue organization. Therefore it is wise to create, when possible, longer jobs. One needs to find the correct balance for how to organizing jobs: if these are too long and fail because of some issue, than a lot of time and resources have been wasted, but such problems can be overcome by tracking the outputs of each step to avoid rerunning all computations. For example, at each step of a job outputting something relevant, there can be a condition checking if the specific output is already present.","title":"Many short jobs running"},{"location":"best/#massive-standard-outputs","text":"Try to avoid printing many outputs on the standard output ( stdout ), in other words a large amount of printed outputs directly to the terminal screen. This can be problematic when a lot of parallel jobs are running, letting stdout filling all the home directory up, and causing errors and eventual data loss. Instead use an output in software-specific data structures (such as .RData files for the R language) or at least simple text files.","title":"Massive standard outputs"},{"location":"best/#packaging-a-coding-project","text":"When coding a piece of software in which there are multiple newly implemented functions, it can be smart to organize all those functions as a package, that can be reused and eventually shared with ease. Such a practice is especially easy and can be mastered very quickly for coding projects in python and R . Link Description pyPA python packaging user guide R package development Develop an R package using Rstudio","title":"Packaging a coding project"},{"location":"best/#pipe-lining-and-submitting-jobs-in-slurm","text":"Slurm is a job scheduler. It allows a user to specify a series of commands and resources requirements to run such commands. Slurm does consider the job submission on an HPC system together with all the other jobs, and prioritize them among other things according to the resources requirement and the available computational power. In figure above, the priority assigned to a Slurm job when the requested time increases, by keeping the memory and CPUs fixed. Decreased priority has higher values. Adapted from A Slurm Simulator: Implementation and Parametric Analysis. Simakov et al 2017. The Danish national HPCs, and most of the other EuroHPC supercomputers, use Slurm as job manager. Link Description Interactive Slurm An interactive Slurm tutorial, in which you will be guided through the process of using Slurm. SLURM example 1 and SLURM example 2 Some examples of how to make a Slurm script to submit a job from the danish HPC GenomeDK and from Princeton Research Computing. Gwf, a simple python tool to create interdependent job submissions Gwf, developed at the University of Aarhus, makes it easy to create Slurm jobs and organize them as a pipeline with dependencies, using the python language (you need python 3.5+). You get to simply create the shell scripts and the dependencies, without the complicating syntax of Slurm. The page contains also a useful guide.","title":"Pipe-lining and submitting jobs in Slurm"},{"location":"best/#version-control","text":"Version control is the tracking of your development history for a project. This allows multiple people working on the same material to keep changes in sync without stepping over each other's contributions. Version control tools allow to commit changes with a description, set up and assign project objectives, open software issues from users and contributors, test automatically the code to find bugs before users step into them. Version control is useful for both teams and single users, and it is a good practice to have version control as a standard for any project. Link Description GitHub the most used tool for version control Github 101 quick introduction to get started on Github GitLab and BitBucket Two other popular alternatives to Github","title":"Version control"},{"location":"choice/","text":"Here you can quickly fill in an assessment form to understand which HPC is best suited to your application. Some Answers are not mandatory in case you are uncertain of some specific choice. initIframe('6054ab3235c7a015b065b8bc');","title":"Choose your HPC"},{"location":"getstarted/","text":"Basics of HPC HPC (High Performance Computing) consists in clustering together a large amount of computing hardware, so that a large number of operations can be executed at once. A supercomputer consists of different types of hardware. In general, hardware is structured in this hierarchy: CPU , the unit that can execute a single sequence of instructions. A CPU can consists of multiple cores, so that multiple chains of instructions can be executed independently. Node , one of the computers that are installed into an HPC system. Cluster , a group of nodes that are connected together and therefore able to communicate and work together on a single task. Moreover, a storage section connected all with one or more types of storage hardware is present in an HPC. A node can consists of only one or more CPUs and some RAM memory. There are other types of nodes containing different hardware combinations. The most common hardware that can be found in a node beyond RAM and CPUs is: GPU , a graphics card. This type of hardware was used for gaming and graphics software, but it has build up a lot of computational power capable of hundreds of parallel processes. This is particularly useful for specific types of linear algebra operations that require the same task to be performed repeatedly. Nvidia and AMD are the main GPU producers. FPGA , a programmable piece of hardware that can do specific operations many times faster than the other available solutions. It can be used to accelerate specific processes that are usually carried out using CPUs. Access to HPC HPC systems allow many users to log into the system at the same time and use part of those resources, usually after they are assigned by an administrator to each user (so that using more resources than assigned will result in stopping whatever software is executed at that time). In Denmark, you have two ways of logging into an HPC system: the first is through a user-friendly interactive interface ( DeiC facility for interactive HPC ), the second is through a classic command line, that requires some knowledge of the UNIX shell language ( here is a good introduction to the Linux shell ). Usually, a user can access to a so-called login node : this has few computational resources allowing the user to login and perform basic operations (small code testing, file management). A user can get assigned a number of CPUs and eventually GPUs/FPGAs/... an amount of RAM a quantity of storage an amount of total time those resources need be used When using DeiC's facility for interactive HPC, the user asks for resources directly through the dashboard once logged in. You will have the chance to exchange messages with the front office responsible for resource assignment, in case your resource demand is too low or too high. However, we suggest to contact first your local front office or eurocc@listserv.deic.dk (HPC facilitation responsible) to get help in submitting a request to obtain resources. For using non-interactive HPCs, you will need to contact the local front office and discuss the possibility of getting resources. What can I use HPC for HPC systems have a large amount of computational power, but this does not mean they are only to be used for large scale projects. You can indeed request entire nodes as well as a single CPU with some GB of RAM. The Danish HPCs are available for any academic application: research projects student projects student exercises in classroom teaching/lecturing Students are not authorized to ask for resources. It will be responsibility of the lecturer/professor to obtain resources through the front office or facilitator. Any student can then be invited/authorized in accessing the project whose resources have been allocated to. Heavy focus of the Danish HPC ecosystem is on teaching and the training of new users, so applications for resources related to courses and students projects are very much welcomed. Advantages of using HPC Using HPC offers many advantages, not only limited to the resources available. In general, using HPC makes the difference in relation to getting a large amount of resources at a cost much lower than buying/owning your own powerful workstation sharing data and settings with other people in collaborative projects using software that is already installed or manageable through a package software: save time instead of configuring and adjusting things on your computer! This is an important aspect especially in teaching, where students have different OS, software versions, and problems with packages. getting technical support from the help desk without being on your own convenience in the sense that computations do not occupy resources on the user's personal computer that is then free to perform other tasks.","title":"HPC 101"},{"location":"getstarted/#basics-of-hpc","text":"HPC (High Performance Computing) consists in clustering together a large amount of computing hardware, so that a large number of operations can be executed at once. A supercomputer consists of different types of hardware. In general, hardware is structured in this hierarchy: CPU , the unit that can execute a single sequence of instructions. A CPU can consists of multiple cores, so that multiple chains of instructions can be executed independently. Node , one of the computers that are installed into an HPC system. Cluster , a group of nodes that are connected together and therefore able to communicate and work together on a single task. Moreover, a storage section connected all with one or more types of storage hardware is present in an HPC. A node can consists of only one or more CPUs and some RAM memory. There are other types of nodes containing different hardware combinations. The most common hardware that can be found in a node beyond RAM and CPUs is: GPU , a graphics card. This type of hardware was used for gaming and graphics software, but it has build up a lot of computational power capable of hundreds of parallel processes. This is particularly useful for specific types of linear algebra operations that require the same task to be performed repeatedly. Nvidia and AMD are the main GPU producers. FPGA , a programmable piece of hardware that can do specific operations many times faster than the other available solutions. It can be used to accelerate specific processes that are usually carried out using CPUs.","title":"Basics of HPC"},{"location":"getstarted/#access-to-hpc","text":"HPC systems allow many users to log into the system at the same time and use part of those resources, usually after they are assigned by an administrator to each user (so that using more resources than assigned will result in stopping whatever software is executed at that time). In Denmark, you have two ways of logging into an HPC system: the first is through a user-friendly interactive interface ( DeiC facility for interactive HPC ), the second is through a classic command line, that requires some knowledge of the UNIX shell language ( here is a good introduction to the Linux shell ). Usually, a user can access to a so-called login node : this has few computational resources allowing the user to login and perform basic operations (small code testing, file management). A user can get assigned a number of CPUs and eventually GPUs/FPGAs/... an amount of RAM a quantity of storage an amount of total time those resources need be used When using DeiC's facility for interactive HPC, the user asks for resources directly through the dashboard once logged in. You will have the chance to exchange messages with the front office responsible for resource assignment, in case your resource demand is too low or too high. However, we suggest to contact first your local front office or eurocc@listserv.deic.dk (HPC facilitation responsible) to get help in submitting a request to obtain resources. For using non-interactive HPCs, you will need to contact the local front office and discuss the possibility of getting resources.","title":"Access to HPC"},{"location":"getstarted/#what-can-i-use-hpc-for","text":"HPC systems have a large amount of computational power, but this does not mean they are only to be used for large scale projects. You can indeed request entire nodes as well as a single CPU with some GB of RAM. The Danish HPCs are available for any academic application: research projects student projects student exercises in classroom teaching/lecturing Students are not authorized to ask for resources. It will be responsibility of the lecturer/professor to obtain resources through the front office or facilitator. Any student can then be invited/authorized in accessing the project whose resources have been allocated to. Heavy focus of the Danish HPC ecosystem is on teaching and the training of new users, so applications for resources related to courses and students projects are very much welcomed.","title":"What can I use HPC for"},{"location":"getstarted/#advantages-of-using-hpc","text":"Using HPC offers many advantages, not only limited to the resources available. In general, using HPC makes the difference in relation to getting a large amount of resources at a cost much lower than buying/owning your own powerful workstation sharing data and settings with other people in collaborative projects using software that is already installed or manageable through a package software: save time instead of configuring and adjusting things on your computer! This is an important aspect especially in teaching, where students have different OS, software versions, and problems with packages. getting technical support from the help desk without being on your own convenience in the sense that computations do not occupy resources on the user's personal computer that is then free to perform other tasks.","title":"Advantages of using HPC"},{"location":"hpc_types/","text":"HPC types DeiC Interactive HPC (UCloud) DeiC Interactive HPC targets users who desire an interactive approach to HPC which ensures that the user's experience is as close to that of a laptop or desktop computer as possible. The computer is accessed through a browser from where a large range of applications can be used. Programs are run through a job which is scheduled through an interactive menu. Once the job is active the user can run the application, for which the job was created, either through the interactive editor, just like on an ordinary laptop or desktop computer, or through the terminal. The simplicity of use makes it ideal for new users, including students, and the large number of available applications provide experienced users with a vast array of tools for their projects. It is therefore well suited for both new and experienced HPC users. With DeiC Interactive HPC users have access to both ordinary CPU, large memory CPU and GPU nodes. More information on DeiC Interactive HPC is available at interactivehpc.dk and access can be found here UCloud (requires WQYF login). DeiC Throughput HPC DeiC Throughput HPC is the traditional HPC setup in which the user accesses a Linux server through an SSH connection. On the server everything is run through the terminal. Specifically the user accesses a front end node from where jobs can be submitted to a queue of jobs managed by a scheduling system, usually Slurm *, which controls resource allocations and starts jobs in due time. Once a job is finished the output is returned to the user. Each job runs autonomously and the user can submit a detailed resource request tailored to each job. Users have access to both ordinary CPU, large memory CPU and GPU nodes. Throughput HPC can handle large amounts of data stored with high levels of security if necessary and is ideal for throughput intensive tasks that can be distributed among multiple cores and/or nodes. The fact that everything is handled through a Linux server using a scheduling system does however imply a steeper learning curve for new users relative to UCloud. DeiC Large Memory HPC Like DeiC Throughput HPC, DeiC Large Memory HPC is configured as a Linux server with the Slurm * scheduler accessed by the user through an SSH connection. DeiC Large Memory HPC however distinguishes itself from DeiC Throughput HPC in the hardware employed by offering a comparatively small number of nodes and CPU cores that have access to large amounts of fast memory. This setup tailors to tasks that cannot efficiently be distributed among cores and nodes or whose memory requirements exceed that offered by DeiC Throughput HPC. The documentation for the computer is available at docs.hpc-type3.sdu.dk . DeiC Accelerated HPC Like the use of GPUs have massively increased the possibilities for massively parallel tasks, extensive research is currently devoted to other types of hardware that can accelerate specific operations. DeiC Accelerated HPC is a testing ground to explore hardware accelerated solutions targetted at future HPC use. A leading hardware component for this type of research is the Field-Programmable Gate Array (FPGA). Where GPUs can accelerate massively parallel tasks as it is hardware designed specifically for such tasks, the goal is for FPGAs is to be hardware configurable to a specific task. In this way one can tackle a prohibiting bottleneck by optimization at the hardware level. Another approach to acceleration is in-memory computing which aims to tackle the following problem: Contemporary big data sets are becoming ever larger and currently exceed even that largest RAM units. This implies that data transfer between hard-drives or flash-drives and memory create a significant bottleneck for contemporary big data programs. In-memory computing aims to solve this problem by distributing the data amongst multiple RAM units each with its own CPU operating in parallel. This requires the use of software to communicate between CPUs in an efficient manner. Capability HPC (LUMI pre-exascale) Capability HPC provides a similar setup to DeiC Throughput HPC but with increased possibilities by virtue of state-of-the-art hardware. Specifically the interconnections between compute nodes is designed to minimize latency thereby addressing the issue of communication induced latency in distributed-memory programs running on separate nodes. Additionally the user can obtain access to large amounts of disk space also with low-latency interconnects. In this way Capability HPC enables computations that are prohibitive with DeiC Throughput HPC due to communication latency. Currently Capability HPC consists of the LUMI pre-exascale computer and once they are operational also Leonardo and MareNostrum5 . These machines are a part of the EuroHPC project where the international collaboration allows the purchase of otherwise inaccessible hardware. In particular LUMI will provide an extensive GPU partition designed to suit machine learning and artificial intelligence applications. * Best practices including an interactive Slurm tutorial are available under section Pipe-lining and submitting jobs in Slurm","title":"HPC types"},{"location":"hpc_types/#hpc-types","text":"","title":"HPC types"},{"location":"hpc_types/#deic-interactive-hpc-ucloud","text":"DeiC Interactive HPC targets users who desire an interactive approach to HPC which ensures that the user's experience is as close to that of a laptop or desktop computer as possible. The computer is accessed through a browser from where a large range of applications can be used. Programs are run through a job which is scheduled through an interactive menu. Once the job is active the user can run the application, for which the job was created, either through the interactive editor, just like on an ordinary laptop or desktop computer, or through the terminal. The simplicity of use makes it ideal for new users, including students, and the large number of available applications provide experienced users with a vast array of tools for their projects. It is therefore well suited for both new and experienced HPC users. With DeiC Interactive HPC users have access to both ordinary CPU, large memory CPU and GPU nodes. More information on DeiC Interactive HPC is available at interactivehpc.dk and access can be found here UCloud (requires WQYF login).","title":"DeiC Interactive HPC (UCloud)"},{"location":"hpc_types/#deic-throughput-hpc","text":"DeiC Throughput HPC is the traditional HPC setup in which the user accesses a Linux server through an SSH connection. On the server everything is run through the terminal. Specifically the user accesses a front end node from where jobs can be submitted to a queue of jobs managed by a scheduling system, usually Slurm *, which controls resource allocations and starts jobs in due time. Once a job is finished the output is returned to the user. Each job runs autonomously and the user can submit a detailed resource request tailored to each job. Users have access to both ordinary CPU, large memory CPU and GPU nodes. Throughput HPC can handle large amounts of data stored with high levels of security if necessary and is ideal for throughput intensive tasks that can be distributed among multiple cores and/or nodes. The fact that everything is handled through a Linux server using a scheduling system does however imply a steeper learning curve for new users relative to UCloud.","title":"DeiC Throughput HPC"},{"location":"hpc_types/#deic-large-memory-hpc","text":"Like DeiC Throughput HPC, DeiC Large Memory HPC is configured as a Linux server with the Slurm * scheduler accessed by the user through an SSH connection. DeiC Large Memory HPC however distinguishes itself from DeiC Throughput HPC in the hardware employed by offering a comparatively small number of nodes and CPU cores that have access to large amounts of fast memory. This setup tailors to tasks that cannot efficiently be distributed among cores and nodes or whose memory requirements exceed that offered by DeiC Throughput HPC. The documentation for the computer is available at docs.hpc-type3.sdu.dk .","title":"DeiC Large Memory HPC"},{"location":"hpc_types/#deic-accelerated-hpc","text":"Like the use of GPUs have massively increased the possibilities for massively parallel tasks, extensive research is currently devoted to other types of hardware that can accelerate specific operations. DeiC Accelerated HPC is a testing ground to explore hardware accelerated solutions targetted at future HPC use. A leading hardware component for this type of research is the Field-Programmable Gate Array (FPGA). Where GPUs can accelerate massively parallel tasks as it is hardware designed specifically for such tasks, the goal is for FPGAs is to be hardware configurable to a specific task. In this way one can tackle a prohibiting bottleneck by optimization at the hardware level. Another approach to acceleration is in-memory computing which aims to tackle the following problem: Contemporary big data sets are becoming ever larger and currently exceed even that largest RAM units. This implies that data transfer between hard-drives or flash-drives and memory create a significant bottleneck for contemporary big data programs. In-memory computing aims to solve this problem by distributing the data amongst multiple RAM units each with its own CPU operating in parallel. This requires the use of software to communicate between CPUs in an efficient manner.","title":"DeiC Accelerated HPC"},{"location":"hpc_types/#capability-hpc-lumi-pre-exascale","text":"Capability HPC provides a similar setup to DeiC Throughput HPC but with increased possibilities by virtue of state-of-the-art hardware. Specifically the interconnections between compute nodes is designed to minimize latency thereby addressing the issue of communication induced latency in distributed-memory programs running on separate nodes. Additionally the user can obtain access to large amounts of disk space also with low-latency interconnects. In this way Capability HPC enables computations that are prohibitive with DeiC Throughput HPC due to communication latency. Currently Capability HPC consists of the LUMI pre-exascale computer and once they are operational also Leonardo and MareNostrum5 . These machines are a part of the EuroHPC project where the international collaboration allows the purchase of otherwise inaccessible hardware. In particular LUMI will provide an extensive GPU partition designed to suit machine learning and artificial intelligence applications. * Best practices including an interactive Slurm tutorial are available under section Pipe-lining and submitting jobs in Slurm","title":"Capability HPC (LUMI pre-exascale)"},{"location":"progr/","text":"As with any other computer, an HPC system can be used with sequential programming. This is the practice of writing computer programs executing one instruction after the other, but not instructions simultaneously in parallel, i.e., parallel programming. Parallel programming There are different ways of writing parallelized code, while in general there is only one way to write sequential code, generally as a logic sequence of steps. OpenMP (multithreading) A popular way of parallel programming is through writing sequential code and pointing at specific pieces of code that can be parallelized into threads (fork-join mechanism, see figure below inspired from ADMIN magazine ). A thread is an independent execution of code with its own allocated memory. If threads vary in execution time, when they have to be joined together to collect data, some threads might have to wait for others, leading to loss of execution time. It is up to the programmer to best balance the distribution of threads to optimize execution times when possible. Modern CPUs support openMP in a natural way, since they are usually multicore CPUs and each core can execute threads independently. OpenMP is available as an extension to the programming languages C and Fortran and is mostly used to parallelize for loops that constitute a time bottleneck for the software execution. Link Description Video course a video course (here the link to the first lesson, you will be able to find all the other lessons associated to that) held by ARCHER UK. OpenMP Starter A starting guide for OpenMP WikiToLearn course An OpenMP course from WikiToLearn MIT course A course from MIT including also MPI usage (next section for more info about MP) MPI (message passing interface) MPI is used to distribute data to different processes, that otherwise could not access to such data (figure below inspired by LLNL ). MPI is considered a very hard language to learn, but this reputation is mostly due to the fact that the message passing is programmed explicitly. Link Description Video course a video course (here the link to the first lesson, you will be able to find all the other lessons associated to that) held by ARCHER UK. MPI Starter A starting guide for OpenMP PRACE course A PRACE course on the MOOC platform FutureLearn GPU programming GPUs (graphical processing units) are computing accelerators that are used to boosts heavy linear algebra applications, such as deep learning. A GPU usually features a large number of special processing units that enable massively parallel execute of code (figure below inspired by Astronomy Computing Today ). When utilized properly GPUs can enable performance of certain programs that is unachievable using only CPUs. To achieve this performance GPUs employ an architecture that is vastly different from that of a CPU which is illustrated below (figure inspired by omisci ), where ALU is an acronym for arithmetic logic unit. For the sake of clarity text is omitted in parts of the figure with the different parts indicated by their colour code. When writing programs that utilize GPUs the different architecture of a GPU poses a different set of challenges compared to those of writing parallel programs for CPUs. Specifically as memory on GPUs is limited compared to CPUs a different approach to memory management is required. AMD and Nvidia are the two main producers of GPUs, where the latter has dominated the market for a long time. The Danish HPCs Type 1 and 2 feature various models of Nvidia graphic cards, while Type 5 (LUMI) has the latest AMD Instinct. The distinction between AMD and Nvidia is mainly due to the fact that they are programmed with two different dialects, and software with dedicated multithreading on GPUs need to be coded specifically for the two brands of GPUs. Nvidia CUDA CUDA is a C++ dialect that has also various library for the most popular languages and packages (e.g. Python , PyTorch , MATLAB , ...). Link Description Nvidia developer training Nvidia developer trainings for CUDA programming Book archive An archive of books for CUDA programming Advanced books Some advanced books for coding with CUDA PyCUDA Code in CUDA with python AMD HIP HIP is a dialect for AMD GPUs of recent introduction. It has the advantage of being able to be compiled for both AMD and Nvidia hardware. CUDA code can be converted to HIP code almost automatically with some extra adjustments by the programmer. The LUMI HPC consortia has already organized a course for HIP coding and CUDA-to-HIP conversion. Check out their page for new courses. Link Description Video introduction 1 Video introduction to HIP Video introduction 2 Video introduction to HIP AMD programming guide Programming guide to HIP from the producer AMD","title":"HPC programming"},{"location":"progr/#parallel-programming","text":"There are different ways of writing parallelized code, while in general there is only one way to write sequential code, generally as a logic sequence of steps.","title":"Parallel programming"},{"location":"progr/#openmp-multithreading","text":"A popular way of parallel programming is through writing sequential code and pointing at specific pieces of code that can be parallelized into threads (fork-join mechanism, see figure below inspired from ADMIN magazine ). A thread is an independent execution of code with its own allocated memory. If threads vary in execution time, when they have to be joined together to collect data, some threads might have to wait for others, leading to loss of execution time. It is up to the programmer to best balance the distribution of threads to optimize execution times when possible. Modern CPUs support openMP in a natural way, since they are usually multicore CPUs and each core can execute threads independently. OpenMP is available as an extension to the programming languages C and Fortran and is mostly used to parallelize for loops that constitute a time bottleneck for the software execution. Link Description Video course a video course (here the link to the first lesson, you will be able to find all the other lessons associated to that) held by ARCHER UK. OpenMP Starter A starting guide for OpenMP WikiToLearn course An OpenMP course from WikiToLearn MIT course A course from MIT including also MPI usage (next section for more info about MP)","title":"OpenMP (multithreading)"},{"location":"progr/#mpi-message-passing-interface","text":"MPI is used to distribute data to different processes, that otherwise could not access to such data (figure below inspired by LLNL ). MPI is considered a very hard language to learn, but this reputation is mostly due to the fact that the message passing is programmed explicitly. Link Description Video course a video course (here the link to the first lesson, you will be able to find all the other lessons associated to that) held by ARCHER UK. MPI Starter A starting guide for OpenMP PRACE course A PRACE course on the MOOC platform FutureLearn","title":"MPI (message passing interface)"},{"location":"progr/#gpu-programming","text":"GPUs (graphical processing units) are computing accelerators that are used to boosts heavy linear algebra applications, such as deep learning. A GPU usually features a large number of special processing units that enable massively parallel execute of code (figure below inspired by Astronomy Computing Today ). When utilized properly GPUs can enable performance of certain programs that is unachievable using only CPUs. To achieve this performance GPUs employ an architecture that is vastly different from that of a CPU which is illustrated below (figure inspired by omisci ), where ALU is an acronym for arithmetic logic unit. For the sake of clarity text is omitted in parts of the figure with the different parts indicated by their colour code. When writing programs that utilize GPUs the different architecture of a GPU poses a different set of challenges compared to those of writing parallel programs for CPUs. Specifically as memory on GPUs is limited compared to CPUs a different approach to memory management is required. AMD and Nvidia are the two main producers of GPUs, where the latter has dominated the market for a long time. The Danish HPCs Type 1 and 2 feature various models of Nvidia graphic cards, while Type 5 (LUMI) has the latest AMD Instinct. The distinction between AMD and Nvidia is mainly due to the fact that they are programmed with two different dialects, and software with dedicated multithreading on GPUs need to be coded specifically for the two brands of GPUs.","title":"GPU programming"},{"location":"progr/#nvidia-cuda","text":"CUDA is a C++ dialect that has also various library for the most popular languages and packages (e.g. Python , PyTorch , MATLAB , ...). Link Description Nvidia developer training Nvidia developer trainings for CUDA programming Book archive An archive of books for CUDA programming Advanced books Some advanced books for coding with CUDA PyCUDA Code in CUDA with python","title":"Nvidia CUDA"},{"location":"progr/#amd-hip","text":"HIP is a dialect for AMD GPUs of recent introduction. It has the advantage of being able to be compiled for both AMD and Nvidia hardware. CUDA code can be converted to HIP code almost automatically with some extra adjustments by the programmer. The LUMI HPC consortia has already organized a course for HIP coding and CUDA-to-HIP conversion. Check out their page for new courses. Link Description Video introduction 1 Video introduction to HIP Video introduction 2 Video introduction to HIP AMD programming guide Programming guide to HIP from the producer AMD","title":"AMD HIP"},{"location":"software/","text":"Software for HPC This page contains useful links to tutorial, learning resources, benchmark and scripts for softwares usable on HPCs. Softwares are separated into broad scientific/application categories. Each tool has small numbers on its right side, indicating on which national HPC types those softwares are already installed. The symbol means that the software is available through the conda package manager for installation, for example, on the GenomeDK Type2 HPC. Remember, you can always contact an HPC front office to inquire about the installation of packages that you need for your applications. ML and AI tensorflow Tensorflow is the Google's open source library used to develop, train and deploy machine learning models. Tensorflow is implemented for python , but now also available in R Links Link Description Tensorflow homepage Tensorflow home page. Contains many tutorials and howtos. Learning material Books and courses in machine learning using tensorflow R guide to tensorflow How to install and use tensorflow in Rstudio pyTorch Pytorch is another Machine Learning framework created mainly by Facebook. Links Link Description PyTorch homepage The official pyTorch homepage, contains tutorials to learn how to use the framework PyTorch book A free book about deep learning with pyTorch PyTorch for deep learning A full video tutorial on PyTorch Astrophysics and Cosmology COSMOMC CosmoMC is a Fortran 2008 Markov-Chain Monte-Carlo (MCMC) engine for exploring cosmological parameter space, together with Fortran and python code for analysing Monte-Carlo samples and importance sampling (plus a suite of scripts for building grids of runs, plotting and presenting results). The code does brute force (but accurate) theoretical matter power spectrum and Cl calculations with CAMB. Links Link Description CosmoMC homepage The official homepage contains the documentation of CosmoMC Introduction to CAMB and CosmoMC A presentation to introduce CosmoMC and CAMB Tutorial for CosmoMC A lecture with usage example of CosmoMC CosmoMC Paper Reference paper of CosmoMC Big data and HPDA KNIME A tool to easily read and transform data, model and visualize it, integrate data science practices and produce insights. Links Link Description Getting started A practical example of data reading and manipulation KNIME platform presentation Slides listing the KNIME properties and tools FORUM Koin the community of users and ask/answer questions and problems KNIME learning A wide range of learning material to use KNIME proficiently TRACER Tracer is graphical tool for visualization and diagnostics of MCMC output. It can read output files from MrBayes and BEAST. Links Link Description TRACER documentation Home page of Beast2 , of which tracer is a tool. It contains tutorials and documentation. Bioinformatics ANGSD A command line tool that integrates many different tools to analyze NGS data efficiently through native C++ implementation, multithreading support and calculation of genotype likelihoods instead of genotype calling. It is easily installed by downloading it from the GitHub repository and compilation of the code as per the instructions in the repository readme file. Links Link Description Github repository Repository for download and installation Documentation Documentation for ANGSD BEDtools Collectively, the bedtools utilities are a swiss-army knife of tools for a wide-range of genomics analysis tasks. The most widely-used tools enable genome arithmetic: that is, set theory on the genome. For example, bedtools allows one to intersect, merge, count, complement, and shuffle genomic intervals from multiple files in widely-used genomic file formats such as BAM, BED, GFF/GTF, VCF. While each individual tool is designed to do a relatively simple task (e.g., intersect two interval files), quite sophisticated analyses can be conducted by combining multiple bedtools operations on the UNIX command line. Links Link Description Bedtools documentation The documentation page. Contains well done tutorials and examples, both for beginners and advanced users. Cellranger The software pipeline used to align single cell sequencing data having Unique Molecular Identifier and coming as output from a 10X sequencing machine. Links Link Description What is cellranger Description of the cellranger pipeline Install cellranger Installation guide Use cellranger User guide and tutorials HOMER HOMER (Hypergeometric Optimization of Motif EnRichment) is a suite of tools for Motif Discovery and next-gen sequencing analysis. It is a collection of command line programs for UNIX-style operating systems written in Perl and C++. HOMER was primarily written as a de novo motif discovery algorithm and is well suited for finding 8-20 bp motifs in large scale genomics data. HOMER contains many useful tools for analyzing ChIP-Seq, GRO-Seq, RNA-Seq, DNase-Seq, Hi-C and numerous other types of functional genomics sequencing data sets. Links Link Description HOMER homepage Software homepage Configure HOMER Setup and configuration Tutorial Tutorial from the official home page Kallisto Kallisto is a program for quantifying abundances of transcripts from bulk and single-cell RNA-Seq data, or more generally of target sequences using high-throughput sequencing reads. It is based on the novel idea of pseudoalignment for rapidly determining the compatibility of reads with targets, without the need for alignment. Links Link Description Homepage Software home page with tutorials and documentation Kallistobus Workflow to use kallisto to generate the expression data matrix Reference The Kallisto paper MACS2 This tool identifies statistically significantly enriched genomic regions in ChIP- and DNase-seq data using the MACS2 algorithm (Model-Based Analysis of ChIP-Seq). Links Link Description Manual MACS2 manual Project page Project page with installation and usage instructions Tutorial A tutorial for peak calling Reference Reference paper Salmon Highly-accurate & wicked fast transcript-level quantification from RNA-seq reads using selective alignment. Alevin, a tool integrated in Salmon, works on 3' tagged data, such as single cell data from 10X machines. Links Link Description Salmon Documentation and tutorial page for Salmon Alevin Documentation and tutorial page for Alevin-Salmon Homepage Software homepage Reference Reference paper SAMtools Samtools is a suite of programs for interacting with high-throughput sequencing data. It consists of three separate repositories: - Samtools , Reading/writing/editing/indexing/viewing SAM/BAM/CRAM format - BCFtools , Reading/writing BCF2/VCF/gVCF files and calling/filtering/summarising SNP and short indel sequence variants - HTSlib , A C library for reading/writing high-throughput sequencing data Links Link Description Homepage Documentation and tutorial page Tutorial 1 Samtools tutorial Tutorial 2 Blogpost on SAMtools Scanpy Scanpy is a scalable toolkit for analyzing single-cell gene expression data built jointly with anndata. It includes preprocessing, visualization, clustering, trajectory inference and differential expression testing. The Python-based implementation efficiently deals with datasets of more than one million cells. Links Link Description Homepage Documentation page with tutorials, package documentation and access to example datasets ready to use Seurat Parallely to Scanpy in python, Seurat is the leading package for single cell data analysis in R. Links Link Description Homepage Documentation page with tutorials, package documentation and access to example datasets ready to use Seqtk Seqtk is a fast and lightweight tool for processing sequences in the FASTA or FASTQ format. It seamlessly parses both FASTA and FASTQ files which can also be optionally compressed by gzip. Links Link Description Homepage Documentation page with command usage Repository and examples Online repository with some usage examples STAR Alignment method utilizes to align sequence reads to the reference genome Links Link Description Homepage STAR software repository Tutorial Online tutorial to use the STAR aligner Computational fluid dynamics openFOAM OpenFOAM is the free, open source CFD software developed primarily by OpenCFD Ltd since 2004. It has a large user base across most areas of engineering and science, from both commercial and academic organisations. OpenFOAM has an extensive range of features to solve anything from complex fluid flows involving chemical reactions, turbulence and heat transfer, to acoustics, solid mechanics and electromagnetics. Links Link Description Homepage , openFOAM foundation Software homepage Forum OpenFOAM users forum Wiki OpenFOAM (unofficial) wiki ANSYS Links Link Description Tutorials for Undergraduate Mechanical Engineering Courses Exercises intended only as an educational tool to assist those who wish to learn how to use ANSYS Basics video tutorial Tutorial for the ANSYS basics Introduction to static structural Video introduction to static structural with ANSYS Simulation of 3d centrifugal pump Video guide to simulate a centrifugal pump with ANSYS Support resources for students Support resources for students. Includes the possibility for a free download of the software for a tryout. University of Alberta - ANSYS Tutorials ANSYS tutorials from the UNiversity of Alberta Computational chemistry GROMACS GROMACS is a versatile package to perform molecular dynamics, i.e. simulate the Newtonian equations of motion for systems with hundreds to millions of particles. It is primarily designed for biochemical molecules like proteins, lipids and nucleic acids that have a lot of complicated bonded interactions, but since GROMACS is extremely fast at calculating the nonbonded interactions (that usually dominate simulations) many groups are also using it for research on non-biological systems, e.g. polymers. Links Link Description GROMACS Official homepage KIGAKI3 tutorial Tutorial with KIGAKI3 peptide model MD tutorials Six tutorials updated to one of the latest GROMACS software versions More Tutorials Six tutorials updated to one of the latest GROMACS software versions Complex MD tutorial Complex and detailed MD tutorial from BioExcel. Requires the BioExcel biobb package NWchem The NWChem software contains computational chemistry tools that are scalable both in their ability to efficiently treat large scientific problems, and in their use of available computing resources from high-performance parallel supercomputers to conventional workstation clusters. NWChem can handle: - Biomolecules, nanostructures, and solid-state - From quantum to classical, and all combinations - Ground and excited-states - Gaussian basis functions or plane-waves - Scaling from one to thousands of processors - Properties and relativistic effects Links Link Description NWCHEM Official homepage with tutorials and installation information Argonne NL tutorial Tutorial from the Argonne national lab LAMMPS LAMMPS is a classical molecular dynamics code with a focus on materials modeling. It's an acronym for Large-scale Atomic/Molecular Massively Parallel Simulator. Links Link Description NWCHEM Official homepage with tutorials and installation information Very Basic Tutorial Basic tutorial with one or more diffusing particles Tutorials for beginners Tutorials for beginners Supramolecular structures Tutorials for modeling supramolecular structure in LAMMPS from Stan Moore, Sandia NL, USA Material from Sandia NL Tutorial collection from the Sandia National Lab's workshops quantumESPRESSO An integrated suite of Open-Source computer codes for electronic-structure calculations and materials modeling at the nanoscale. It is based on density-functional theory, plane waves, and pseudopotentials. Links Link Description QuantumEspresso Homepage with installation, documentation and learning resources QE and GPUs How to use QuantumEspresso on GPU based HPC systems Updated tutorials Some updated tutorials from University of Udine, IT Step by step tutorial A detailed tutorial (based on a specific supercomputing facility) Small tutorial A small QE tutorial SIESTA SIESTA is both a method and its computer program implementation, to perform efficient electronic structure calculations and ab initio molecular dynamics simulations of molecules and solids. SIESTA's efficiency stems from the use of a basis set of strictly-localized atomic orbitals. A very important feature of the code is that its accuracy and cost can be tuned in a wide range, from quick exploratory calculations to highly accurate simulations matching the quality of other approaches, such as plane-wave methods. Links Link Description large tutorial collection A large collection of tutorials for Siesta Tel Aviv Siesta tutorial Siesta Workshop material SIMUNE material A range of tutorials to guide the learning process in using Siesta. Developed by Simune Atomistics Tutorial Repository A big repository with exercises for Siesta Development Dash Dash is a productive Python framework for building web analytic applications. Dash is ideal for building data visualization apps with highly custom user interfaces in pure Python. It's particularly suited for anyone who works with data in Python. Through a couple of simple patterns, Dash abstracts away all of the technologies and protocols that are required to build an interactive web-based application. Dash is simple enough that you can bind a user interface around your Python code in an afternoon. Dash apps are rendered in the web browser. You can deploy your apps to servers and then share them through URLs. Since Dash apps are viewed in the web browser, Dash is inherently cross-platform and mobile ready. Links Link Description Dash homepage The Dash homepage. It contains tutorial for a quick Video introduction A video introduction to Dash Step by step tutorial A step by step tutorial to produce a Dash web application Jupyter Lab JupyterLab is a web-based interactive development environment for Jupyter notebooks, code, and data. JupyterLab is flexible: configure and arrange the user interface to support a wide range of workflows in data science, scientific computing, and machine learning. Link Description Homepage The Homepage of project jupyter Matlab MATLAB\u00ae combines a desktop environment tuned for iterative analysis and design processes with a programming language that expresses matrix and array mathematics directly. It includes the Live Editor for creating scripts that combine code, output, and formatted text in an executable notebook. Link Description Homepage The Homepage of Mathworks Matlab Rstudio An integrated development environment for R and Python, with a console, syntax-highlighting editor that supports direct code execution, and tools for plotting, history, debugging and workspace management. Link Description Homepage The Homepage of Rstudio Visual Studio code Visual Studio Code is a lightweight but powerful source code editor. It comes with built-in support for JavaScript, TypeScript and Node.js and has a rich ecosystem of extensions for other languages (such as C++, C#, Java, Python, PHP, Go) and runtimes (such as .NET and Unity). Manages your Git repository and has a lot of plugins freely installable from the dedicated store. Link Description Homepage The Homepage of Visual Studio Code Spark Apache Spark is a lightning-fast unified analytics engine for big data and machine learning. It was originally developed at UC Berkeley in 2009. Natural sciences and engineering COMSOL COMSOL Multiphysics is a cross-platform finite element analysis, solver and multiphysics simulation software. It allows conventional physics-based user interfaces and coupled systems of partial differential equations (PDEs). COMSOL provides an IDE and unified workflow for electrical, mechanical, fluid, acoustics, and chemical applications. Link Description COMSOL models A gallery of applications from the COMSOL homepage FEniCS FEniCS is a popular open-source (LGPLv3) computing platform for solving partial differential equations (PDEs). FEniCS enables users to quickly translate scientific models into efficient finite element code. With the high-level Python and C++ interfaces to FEniCS, it is easy to get started, but FEniCS offers also powerful capabilities for more experienced programmers. FEniCS runs on a multitude of platforms ranging from laptops to high-performance clusters. Link Description Homepage Homepage of the FEniCS project Tutorials and book Free official tutorials and book FreeCAD FreeCAD is a parametric 3D modeler made primarily to design real-life objects of any size. Link Description Beginners tutorial Beginners video tutorial Free documentation Freecad wiki Probabilistic programming pyMC3 PyMC3 allows you to write down models using an intuitive syntax to describe a data generating process. Cutting edge algorithms and model building blocks Fit your model using gradient-based MCMC algorithms like NUTS, using ADVI for fast approximate inference \u2014 including minibatch-ADVI for scaling to large datasets \u2014 or using Gaussian processes to build Bayesian nonparametric models. Link Description Documentation Documentation page with tutorials, videos, examples and books While my MCMC gently samples A blog about pyMC3 and Bayesian inference Bayesian modeling cookbook blog A blog on bayesian modeling Probabilistic Programming blog A blog on probabilistic programming from a pyMC3 contributor Social Sciences, Humanities NetLogo NetLogo is a multi-agent programmable modeling environment. It is used by many hundreds of thousands of students, teachers, and researchers worldwide. It also powers HubNet participatory simulations. It is authored by Uri Wilensky and developed at the CCL. Link Description Homepage Netlogo homepage with tutorials, extensions, resources. oTree An open-source platform for behavioral research oTree lets you create: - Controlled behavioral experiments in economics, market research, psychology, and related fields - Multiplayer strategy games, like the prisoner's dilemma, public goods game, and auctions. - Surveys and quizzes, especially those that require customized or dynamic functionality not available with conventional survey software. Link Description Homepage oTree homepage with tutorials, extensions, resources.","title":"Software for HPC"},{"location":"software/#software-for-hpc","text":"This page contains useful links to tutorial, learning resources, benchmark and scripts for softwares usable on HPCs. Softwares are separated into broad scientific/application categories. Each tool has small numbers on its right side, indicating on which national HPC types those softwares are already installed. The symbol means that the software is available through the conda package manager for installation, for example, on the GenomeDK Type2 HPC. Remember, you can always contact an HPC front office to inquire about the installation of packages that you need for your applications.","title":"Software for HPC"},{"location":"software/#ml-and-ai","text":"","title":"ML and AI"},{"location":"software/#tensorflow","text":"Tensorflow is the Google's open source library used to develop, train and deploy machine learning models. Tensorflow is implemented for python , but now also available in R Links Link Description Tensorflow homepage Tensorflow home page. Contains many tutorials and howtos. Learning material Books and courses in machine learning using tensorflow R guide to tensorflow How to install and use tensorflow in Rstudio","title":"tensorflow"},{"location":"software/#pytorch","text":"Pytorch is another Machine Learning framework created mainly by Facebook. Links Link Description PyTorch homepage The official pyTorch homepage, contains tutorials to learn how to use the framework PyTorch book A free book about deep learning with pyTorch PyTorch for deep learning A full video tutorial on PyTorch","title":"pyTorch"},{"location":"software/#astrophysics-and-cosmology","text":"","title":"Astrophysics and Cosmology"},{"location":"software/#cosmomc","text":"CosmoMC is a Fortran 2008 Markov-Chain Monte-Carlo (MCMC) engine for exploring cosmological parameter space, together with Fortran and python code for analysing Monte-Carlo samples and importance sampling (plus a suite of scripts for building grids of runs, plotting and presenting results). The code does brute force (but accurate) theoretical matter power spectrum and Cl calculations with CAMB. Links Link Description CosmoMC homepage The official homepage contains the documentation of CosmoMC Introduction to CAMB and CosmoMC A presentation to introduce CosmoMC and CAMB Tutorial for CosmoMC A lecture with usage example of CosmoMC CosmoMC Paper Reference paper of CosmoMC","title":"COSMOMC"},{"location":"software/#big-data-and-hpda","text":"","title":"Big data and HPDA"},{"location":"software/#knime","text":"A tool to easily read and transform data, model and visualize it, integrate data science practices and produce insights. Links Link Description Getting started A practical example of data reading and manipulation KNIME platform presentation Slides listing the KNIME properties and tools FORUM Koin the community of users and ask/answer questions and problems KNIME learning A wide range of learning material to use KNIME proficiently","title":"KNIME"},{"location":"software/#tracer","text":"Tracer is graphical tool for visualization and diagnostics of MCMC output. It can read output files from MrBayes and BEAST. Links Link Description TRACER documentation Home page of Beast2 , of which tracer is a tool. It contains tutorials and documentation.","title":"TRACER"},{"location":"software/#bioinformatics","text":"","title":"Bioinformatics"},{"location":"software/#angsd","text":"A command line tool that integrates many different tools to analyze NGS data efficiently through native C++ implementation, multithreading support and calculation of genotype likelihoods instead of genotype calling. It is easily installed by downloading it from the GitHub repository and compilation of the code as per the instructions in the repository readme file. Links Link Description Github repository Repository for download and installation Documentation Documentation for ANGSD","title":"ANGSD"},{"location":"software/#bedtools","text":"Collectively, the bedtools utilities are a swiss-army knife of tools for a wide-range of genomics analysis tasks. The most widely-used tools enable genome arithmetic: that is, set theory on the genome. For example, bedtools allows one to intersect, merge, count, complement, and shuffle genomic intervals from multiple files in widely-used genomic file formats such as BAM, BED, GFF/GTF, VCF. While each individual tool is designed to do a relatively simple task (e.g., intersect two interval files), quite sophisticated analyses can be conducted by combining multiple bedtools operations on the UNIX command line. Links Link Description Bedtools documentation The documentation page. Contains well done tutorials and examples, both for beginners and advanced users.","title":"BEDtools"},{"location":"software/#cellranger","text":"The software pipeline used to align single cell sequencing data having Unique Molecular Identifier and coming as output from a 10X sequencing machine. Links Link Description What is cellranger Description of the cellranger pipeline Install cellranger Installation guide Use cellranger User guide and tutorials","title":"Cellranger"},{"location":"software/#homer","text":"HOMER (Hypergeometric Optimization of Motif EnRichment) is a suite of tools for Motif Discovery and next-gen sequencing analysis. It is a collection of command line programs for UNIX-style operating systems written in Perl and C++. HOMER was primarily written as a de novo motif discovery algorithm and is well suited for finding 8-20 bp motifs in large scale genomics data. HOMER contains many useful tools for analyzing ChIP-Seq, GRO-Seq, RNA-Seq, DNase-Seq, Hi-C and numerous other types of functional genomics sequencing data sets. Links Link Description HOMER homepage Software homepage Configure HOMER Setup and configuration Tutorial Tutorial from the official home page","title":"HOMER"},{"location":"software/#kallisto","text":"Kallisto is a program for quantifying abundances of transcripts from bulk and single-cell RNA-Seq data, or more generally of target sequences using high-throughput sequencing reads. It is based on the novel idea of pseudoalignment for rapidly determining the compatibility of reads with targets, without the need for alignment. Links Link Description Homepage Software home page with tutorials and documentation Kallistobus Workflow to use kallisto to generate the expression data matrix Reference The Kallisto paper","title":"Kallisto"},{"location":"software/#macs2","text":"This tool identifies statistically significantly enriched genomic regions in ChIP- and DNase-seq data using the MACS2 algorithm (Model-Based Analysis of ChIP-Seq). Links Link Description Manual MACS2 manual Project page Project page with installation and usage instructions Tutorial A tutorial for peak calling Reference Reference paper","title":"MACS2"},{"location":"software/#salmon","text":"Highly-accurate & wicked fast transcript-level quantification from RNA-seq reads using selective alignment. Alevin, a tool integrated in Salmon, works on 3' tagged data, such as single cell data from 10X machines. Links Link Description Salmon Documentation and tutorial page for Salmon Alevin Documentation and tutorial page for Alevin-Salmon Homepage Software homepage Reference Reference paper","title":"Salmon"},{"location":"software/#samtools","text":"Samtools is a suite of programs for interacting with high-throughput sequencing data. It consists of three separate repositories: - Samtools , Reading/writing/editing/indexing/viewing SAM/BAM/CRAM format - BCFtools , Reading/writing BCF2/VCF/gVCF files and calling/filtering/summarising SNP and short indel sequence variants - HTSlib , A C library for reading/writing high-throughput sequencing data Links Link Description Homepage Documentation and tutorial page Tutorial 1 Samtools tutorial Tutorial 2 Blogpost on SAMtools","title":"SAMtools"},{"location":"software/#scanpy","text":"Scanpy is a scalable toolkit for analyzing single-cell gene expression data built jointly with anndata. It includes preprocessing, visualization, clustering, trajectory inference and differential expression testing. The Python-based implementation efficiently deals with datasets of more than one million cells. Links Link Description Homepage Documentation page with tutorials, package documentation and access to example datasets ready to use","title":"Scanpy"},{"location":"software/#seurat","text":"Parallely to Scanpy in python, Seurat is the leading package for single cell data analysis in R. Links Link Description Homepage Documentation page with tutorials, package documentation and access to example datasets ready to use","title":"Seurat"},{"location":"software/#seqtk","text":"Seqtk is a fast and lightweight tool for processing sequences in the FASTA or FASTQ format. It seamlessly parses both FASTA and FASTQ files which can also be optionally compressed by gzip. Links Link Description Homepage Documentation page with command usage Repository and examples Online repository with some usage examples","title":"Seqtk"},{"location":"software/#star","text":"Alignment method utilizes to align sequence reads to the reference genome Links Link Description Homepage STAR software repository Tutorial Online tutorial to use the STAR aligner","title":"STAR"},{"location":"software/#computational-fluid-dynamics","text":"","title":"Computational fluid dynamics"},{"location":"software/#openfoam","text":"OpenFOAM is the free, open source CFD software developed primarily by OpenCFD Ltd since 2004. It has a large user base across most areas of engineering and science, from both commercial and academic organisations. OpenFOAM has an extensive range of features to solve anything from complex fluid flows involving chemical reactions, turbulence and heat transfer, to acoustics, solid mechanics and electromagnetics. Links Link Description Homepage , openFOAM foundation Software homepage Forum OpenFOAM users forum Wiki OpenFOAM (unofficial) wiki","title":"openFOAM"},{"location":"software/#ansys","text":"Links Link Description Tutorials for Undergraduate Mechanical Engineering Courses Exercises intended only as an educational tool to assist those who wish to learn how to use ANSYS Basics video tutorial Tutorial for the ANSYS basics Introduction to static structural Video introduction to static structural with ANSYS Simulation of 3d centrifugal pump Video guide to simulate a centrifugal pump with ANSYS Support resources for students Support resources for students. Includes the possibility for a free download of the software for a tryout. University of Alberta - ANSYS Tutorials ANSYS tutorials from the UNiversity of Alberta","title":"ANSYS"},{"location":"software/#computational-chemistry","text":"","title":"Computational chemistry"},{"location":"software/#gromacs","text":"GROMACS is a versatile package to perform molecular dynamics, i.e. simulate the Newtonian equations of motion for systems with hundreds to millions of particles. It is primarily designed for biochemical molecules like proteins, lipids and nucleic acids that have a lot of complicated bonded interactions, but since GROMACS is extremely fast at calculating the nonbonded interactions (that usually dominate simulations) many groups are also using it for research on non-biological systems, e.g. polymers. Links Link Description GROMACS Official homepage KIGAKI3 tutorial Tutorial with KIGAKI3 peptide model MD tutorials Six tutorials updated to one of the latest GROMACS software versions More Tutorials Six tutorials updated to one of the latest GROMACS software versions Complex MD tutorial Complex and detailed MD tutorial from BioExcel. Requires the BioExcel biobb package","title":"GROMACS"},{"location":"software/#nwchem","text":"The NWChem software contains computational chemistry tools that are scalable both in their ability to efficiently treat large scientific problems, and in their use of available computing resources from high-performance parallel supercomputers to conventional workstation clusters. NWChem can handle: - Biomolecules, nanostructures, and solid-state - From quantum to classical, and all combinations - Ground and excited-states - Gaussian basis functions or plane-waves - Scaling from one to thousands of processors - Properties and relativistic effects Links Link Description NWCHEM Official homepage with tutorials and installation information Argonne NL tutorial Tutorial from the Argonne national lab","title":"NWchem"},{"location":"software/#lammps","text":"LAMMPS is a classical molecular dynamics code with a focus on materials modeling. It's an acronym for Large-scale Atomic/Molecular Massively Parallel Simulator. Links Link Description NWCHEM Official homepage with tutorials and installation information Very Basic Tutorial Basic tutorial with one or more diffusing particles Tutorials for beginners Tutorials for beginners Supramolecular structures Tutorials for modeling supramolecular structure in LAMMPS from Stan Moore, Sandia NL, USA Material from Sandia NL Tutorial collection from the Sandia National Lab's workshops","title":"LAMMPS"},{"location":"software/#quantumespresso","text":"An integrated suite of Open-Source computer codes for electronic-structure calculations and materials modeling at the nanoscale. It is based on density-functional theory, plane waves, and pseudopotentials. Links Link Description QuantumEspresso Homepage with installation, documentation and learning resources QE and GPUs How to use QuantumEspresso on GPU based HPC systems Updated tutorials Some updated tutorials from University of Udine, IT Step by step tutorial A detailed tutorial (based on a specific supercomputing facility) Small tutorial A small QE tutorial","title":"quantumESPRESSO"},{"location":"software/#siesta","text":"SIESTA is both a method and its computer program implementation, to perform efficient electronic structure calculations and ab initio molecular dynamics simulations of molecules and solids. SIESTA's efficiency stems from the use of a basis set of strictly-localized atomic orbitals. A very important feature of the code is that its accuracy and cost can be tuned in a wide range, from quick exploratory calculations to highly accurate simulations matching the quality of other approaches, such as plane-wave methods. Links Link Description large tutorial collection A large collection of tutorials for Siesta Tel Aviv Siesta tutorial Siesta Workshop material SIMUNE material A range of tutorials to guide the learning process in using Siesta. Developed by Simune Atomistics Tutorial Repository A big repository with exercises for Siesta","title":"SIESTA"},{"location":"software/#development","text":"","title":"Development"},{"location":"software/#dash","text":"Dash is a productive Python framework for building web analytic applications. Dash is ideal for building data visualization apps with highly custom user interfaces in pure Python. It's particularly suited for anyone who works with data in Python. Through a couple of simple patterns, Dash abstracts away all of the technologies and protocols that are required to build an interactive web-based application. Dash is simple enough that you can bind a user interface around your Python code in an afternoon. Dash apps are rendered in the web browser. You can deploy your apps to servers and then share them through URLs. Since Dash apps are viewed in the web browser, Dash is inherently cross-platform and mobile ready. Links Link Description Dash homepage The Dash homepage. It contains tutorial for a quick Video introduction A video introduction to Dash Step by step tutorial A step by step tutorial to produce a Dash web application","title":"Dash"},{"location":"software/#jupyter-lab","text":"JupyterLab is a web-based interactive development environment for Jupyter notebooks, code, and data. JupyterLab is flexible: configure and arrange the user interface to support a wide range of workflows in data science, scientific computing, and machine learning. Link Description Homepage The Homepage of project jupyter","title":"Jupyter Lab"},{"location":"software/#matlab","text":"MATLAB\u00ae combines a desktop environment tuned for iterative analysis and design processes with a programming language that expresses matrix and array mathematics directly. It includes the Live Editor for creating scripts that combine code, output, and formatted text in an executable notebook. Link Description Homepage The Homepage of Mathworks Matlab","title":"Matlab"},{"location":"software/#rstudio","text":"An integrated development environment for R and Python, with a console, syntax-highlighting editor that supports direct code execution, and tools for plotting, history, debugging and workspace management. Link Description Homepage The Homepage of Rstudio","title":"Rstudio"},{"location":"software/#visual-studio-code","text":"Visual Studio Code is a lightweight but powerful source code editor. It comes with built-in support for JavaScript, TypeScript and Node.js and has a rich ecosystem of extensions for other languages (such as C++, C#, Java, Python, PHP, Go) and runtimes (such as .NET and Unity). Manages your Git repository and has a lot of plugins freely installable from the dedicated store. Link Description Homepage The Homepage of Visual Studio Code","title":"Visual Studio code"},{"location":"software/#spark","text":"Apache Spark is a lightning-fast unified analytics engine for big data and machine learning. It was originally developed at UC Berkeley in 2009.","title":"Spark"},{"location":"software/#natural-sciences-and-engineering","text":"","title":"Natural sciences and engineering"},{"location":"software/#comsol","text":"COMSOL Multiphysics is a cross-platform finite element analysis, solver and multiphysics simulation software. It allows conventional physics-based user interfaces and coupled systems of partial differential equations (PDEs). COMSOL provides an IDE and unified workflow for electrical, mechanical, fluid, acoustics, and chemical applications. Link Description COMSOL models A gallery of applications from the COMSOL homepage","title":"COMSOL"},{"location":"software/#fenics","text":"FEniCS is a popular open-source (LGPLv3) computing platform for solving partial differential equations (PDEs). FEniCS enables users to quickly translate scientific models into efficient finite element code. With the high-level Python and C++ interfaces to FEniCS, it is easy to get started, but FEniCS offers also powerful capabilities for more experienced programmers. FEniCS runs on a multitude of platforms ranging from laptops to high-performance clusters. Link Description Homepage Homepage of the FEniCS project Tutorials and book Free official tutorials and book","title":"FEniCS"},{"location":"software/#freecad","text":"FreeCAD is a parametric 3D modeler made primarily to design real-life objects of any size. Link Description Beginners tutorial Beginners video tutorial Free documentation Freecad wiki","title":"FreeCAD"},{"location":"software/#probabilistic-programming","text":"","title":"Probabilistic programming"},{"location":"software/#pymc3","text":"PyMC3 allows you to write down models using an intuitive syntax to describe a data generating process. Cutting edge algorithms and model building blocks Fit your model using gradient-based MCMC algorithms like NUTS, using ADVI for fast approximate inference \u2014 including minibatch-ADVI for scaling to large datasets \u2014 or using Gaussian processes to build Bayesian nonparametric models. Link Description Documentation Documentation page with tutorials, videos, examples and books While my MCMC gently samples A blog about pyMC3 and Bayesian inference Bayesian modeling cookbook blog A blog on bayesian modeling Probabilistic Programming blog A blog on probabilistic programming from a pyMC3 contributor","title":"pyMC3"},{"location":"software/#social-sciences-humanities","text":"","title":"Social Sciences, Humanities"},{"location":"software/#netlogo","text":"NetLogo is a multi-agent programmable modeling environment. It is used by many hundreds of thousands of students, teachers, and researchers worldwide. It also powers HubNet participatory simulations. It is authored by Uri Wilensky and developed at the CCL. Link Description Homepage Netlogo homepage with tutorials, extensions, resources.","title":"NetLogo"},{"location":"software/#otree","text":"An open-source platform for behavioral research oTree lets you create: - Controlled behavioral experiments in economics, market research, psychology, and related fields - Multiplayer strategy games, like the prisoner's dilemma, public goods game, and auctions. - Surveys and quizzes, especially those that require customized or dynamic functionality not available with conventional survey software. Link Description Homepage oTree homepage with tutorials, extensions, resources.","title":"oTree"},{"location":"use_cases/","text":"Use Cases This page contains a list of research projects that have successfully used HPC resources to showcase the possibilities HPC enables researchers. The Danish Blood Donor Study (DBDS) genomic cohort The DBDS genomic cohort is a comprehensive catalog for large-scale genetic analyses concerning numerous environmental and lifestyle factors affecting donors' health. The study includes data from more than 100.000 donors. It has assessed quality measures such as kinship, ethnicity, and minor allele frequency. The study makes continuous assessments of the donors, resulting in large amounts of data collected and processed using HPC. The use of HPC allows for simultaneously testing multiple hypotheses and dynamical scaling. Hansen, T. F. et al. \"DBDS Genomic Cohort, a prospective and comprehensive resource for integrative and temporal analysis of genetic, environmental and lifestyle factors affecting health of blood donors\" (2019), BMJ Open. HPC in the field: research and recording of prehistoric rock art using 3D image-based modelling This project investigated the mapping between prehistoric rock art images and their meaning. Due to the inherently three-dimensional nature of rock art, the required image processing can require a lot of computing power. By outsourcing the image processing to an HPC facility, the researchers could obtain results while in the field without the need to carry extra computing equipment to the site and make decisions based on the processed data. In this way, HPC enables greater flexibility during fieldwork by reducing the barrier of image processing time without the need for extra equipment on site. Dodd, J. \"The application of high performance computing in rock art documentation and research\" (2018), Adoranten 92-104. Exploring past economies with HPC-enabled agent-based modelling This project studied economic patterns in the Eastern Roman Empire by combining economic models with archaeological data. A significant challenge in such a project is parameter estimation due to the lack of input data. A vast parameter space has to be explored to accurately fit a sufficiently complex model to the available data. This requires a large number of model simulations necessitating advanced computational resources. HPC thus enabled the researchers to study the economic patterns of an ancient society using archaeological data. Carrignon, S. et al. \"Tableware trade in the Roman East: Exploring cultural and economic transmission with agent-based modelling and approximate Bayesian computation\" (2020), PLOS ONE . Inquisitiveness: distribution rational thinking This study aims to redefine bounded reality based on a more socialized view of the individual. To that end, it introduces inquisitiveness to refer to an agent who mainly relies on learning by inquiry. It introduces a more active, forward-looking, and creative side compared to the concept of docility. The authors apply an agent-based simulation to explore the impact of inquisitiveness, where inquisitiveness is seen as the tool that links agents together. However, the simulation turned out to be complex due to the large number of parameters included. Therefore, the full model was only able to run using HPC resources. This simulation showed that inquisitiveness brings more problem-solving efficiency to the system. Bardone, E. and Secchi, D. \"Inquisitiveness: distribution rational thinking\" (2017), Team Performance Management, vol. 23, No 1/2. DaCy \u2013 A Unified Framework for Danish natural language processing (NLP) Natural language processing (NLP), such as recognizing names for anonymization or detecting hate speech on the internet, has seen considerable improvements in recent years; however, only a few Danish models exist, and they use different underlying frameworks. Using UCloud, DaCy has been developed, which besides integrating existing models, also includes multiple new models for syntactical analysis and named entity recognition that obtain state-of-the-art performance for these tasks. The use of UCloud allowed the project to quickly scale up from a few initial models to larger models. This led to noticeable performance increases and obtaining a step change on tasks such as dependency parsing, which can determine who or what an adjective describes. Similarly, using UCloud made it easy to include multiple collaborators in the project, allowing for future projects without creating an infrastructure for secure file management. Enevoldsen, K. et al. \"DaCy: A Unified Framework for Danish NLP\" (2021), arXiv. XPEROHS - Expression and Perceiving Online Hate Speech The XPEROHS-project aims to investigate online hate speech. Its finding involves the semantics and pragmatics of denigration, the covert dynamics of hate speech, perceptions of spoken and written hate speech, and rhetorical hate speech strategies employed in online interaction. The project uses data from large-scale Facebook and Twitter corpora, yielding corpus sizes of hundreds of millions. These enormous amounts of data make linguistic annotation extremely challenging. Therefore, it is essential to the project that this annotation can be performed on HPC clusters rather than laptops. Baumgarten, N. et al. \"Towards Balance and Boundaries in Public Discourse: Expressing and Perceiving Online Hate Speech (XPEROHS)\" (2019), RASK \u2013 International journal of language and communication. Conspicuous consumption in the United States and China Conspicuous consumption describes and explains the consumer practice of buying and using goods of higher quality, price, or greater quantity than practical. This study considered differences in conspicuous consumption in the United States and China. The paper uses a partial-equilibrium, heterogeneous-agent structural model, where the consumer's peer infers the consumer's wealth based on their purchases. The model was simulated and shown to produce data similar to the observed data using HPC. Simulations were also used to estimate welfare gains of sales taxes on various consumer goods. Jinkins, D. \"Conspicuous consumption in the United States and China\" (2016), Journal of Economic Behavior & Organization. Detection of Mycobacterium leprae in archeaological human dental calculus using HPC-enabled sequencing This study is the first to demonstrate the recovery of ancient M. leprae biomolecules from archaeological dental calculus. As dental calculus are pretty robust, it is often conserved over long periods. Therefore, it may represent an alternative sample source to bones and teeth for detecting and molecularly characterizing M. leprae in individuals from the archaeological record, especially in the absence of definitive osteological markers. This is particularly relevant for cases where human remains are poorly preserved or too precious to warrant destructive bone sampling. The M. leprae genome was recovered using shotgun sequencing done using HPC resources. Fotakis, A. K. et al. \"Multi-omic detection of Mycobacterium leprae in archaeological human dental calculus\" (2020), Phil. Trans. R. Soc. B 375: 20190584. HPC-enabled genomic sequencing of dog skin garments Among Indigenous populations of the Arctic, domestic dogs were social actors aiding in traction and subsistence activities. Less commonly, dogs fulfilled a fur-bearing role in both the North American and Siberian Arctic. This study sequenced the mitochondrial genomes of macroscopically identified dog skin garments. This sequencing was made possible using HPC resources. The study found that dog provisioning practices were variable across the Siberian and North American Arctic, but in all cases, involved considerable human labor. Harris, A. J. T. et al. \"Archives of human-dog relationships: Genetic and stable isotope analysis of Arctic fur clothing\" (2020), Journal of Anthropological Archeaology 59, 101200.","title":"HPC use cases"},{"location":"use_cases/#use-cases","text":"This page contains a list of research projects that have successfully used HPC resources to showcase the possibilities HPC enables researchers.","title":"Use Cases"},{"location":"use_cases/#the-danish-blood-donor-study-dbds-genomic-cohort","text":"The DBDS genomic cohort is a comprehensive catalog for large-scale genetic analyses concerning numerous environmental and lifestyle factors affecting donors' health. The study includes data from more than 100.000 donors. It has assessed quality measures such as kinship, ethnicity, and minor allele frequency. The study makes continuous assessments of the donors, resulting in large amounts of data collected and processed using HPC. The use of HPC allows for simultaneously testing multiple hypotheses and dynamical scaling. Hansen, T. F. et al. \"DBDS Genomic Cohort, a prospective and comprehensive resource for integrative and temporal analysis of genetic, environmental and lifestyle factors affecting health of blood donors\" (2019), BMJ Open.","title":"The Danish Blood Donor Study (DBDS) genomic cohort"},{"location":"use_cases/#hpc-in-the-field-research-and-recording-of-prehistoric-rock-art-using-3d-image-based-modelling","text":"This project investigated the mapping between prehistoric rock art images and their meaning. Due to the inherently three-dimensional nature of rock art, the required image processing can require a lot of computing power. By outsourcing the image processing to an HPC facility, the researchers could obtain results while in the field without the need to carry extra computing equipment to the site and make decisions based on the processed data. In this way, HPC enables greater flexibility during fieldwork by reducing the barrier of image processing time without the need for extra equipment on site. Dodd, J. \"The application of high performance computing in rock art documentation and research\" (2018), Adoranten 92-104.","title":"HPC in the field: research and recording of prehistoric rock art using 3D image-based modelling"},{"location":"use_cases/#exploring-past-economies-with-hpc-enabled-agent-based-modelling","text":"This project studied economic patterns in the Eastern Roman Empire by combining economic models with archaeological data. A significant challenge in such a project is parameter estimation due to the lack of input data. A vast parameter space has to be explored to accurately fit a sufficiently complex model to the available data. This requires a large number of model simulations necessitating advanced computational resources. HPC thus enabled the researchers to study the economic patterns of an ancient society using archaeological data. Carrignon, S. et al. \"Tableware trade in the Roman East: Exploring cultural and economic transmission with agent-based modelling and approximate Bayesian computation\" (2020), PLOS ONE .","title":"Exploring past economies with HPC-enabled agent-based modelling"},{"location":"use_cases/#inquisitiveness-distribution-rational-thinking","text":"This study aims to redefine bounded reality based on a more socialized view of the individual. To that end, it introduces inquisitiveness to refer to an agent who mainly relies on learning by inquiry. It introduces a more active, forward-looking, and creative side compared to the concept of docility. The authors apply an agent-based simulation to explore the impact of inquisitiveness, where inquisitiveness is seen as the tool that links agents together. However, the simulation turned out to be complex due to the large number of parameters included. Therefore, the full model was only able to run using HPC resources. This simulation showed that inquisitiveness brings more problem-solving efficiency to the system. Bardone, E. and Secchi, D. \"Inquisitiveness: distribution rational thinking\" (2017), Team Performance Management, vol. 23, No 1/2.","title":"Inquisitiveness: distribution rational thinking"},{"location":"use_cases/#dacy-a-unified-framework-for-danish-natural-language-processing-nlp","text":"Natural language processing (NLP), such as recognizing names for anonymization or detecting hate speech on the internet, has seen considerable improvements in recent years; however, only a few Danish models exist, and they use different underlying frameworks. Using UCloud, DaCy has been developed, which besides integrating existing models, also includes multiple new models for syntactical analysis and named entity recognition that obtain state-of-the-art performance for these tasks. The use of UCloud allowed the project to quickly scale up from a few initial models to larger models. This led to noticeable performance increases and obtaining a step change on tasks such as dependency parsing, which can determine who or what an adjective describes. Similarly, using UCloud made it easy to include multiple collaborators in the project, allowing for future projects without creating an infrastructure for secure file management. Enevoldsen, K. et al. \"DaCy: A Unified Framework for Danish NLP\" (2021), arXiv.","title":"DaCy \u2013 A Unified Framework for Danish natural language processing (NLP)"},{"location":"use_cases/#xperohs-expression-and-perceiving-online-hate-speech","text":"The XPEROHS-project aims to investigate online hate speech. Its finding involves the semantics and pragmatics of denigration, the covert dynamics of hate speech, perceptions of spoken and written hate speech, and rhetorical hate speech strategies employed in online interaction. The project uses data from large-scale Facebook and Twitter corpora, yielding corpus sizes of hundreds of millions. These enormous amounts of data make linguistic annotation extremely challenging. Therefore, it is essential to the project that this annotation can be performed on HPC clusters rather than laptops. Baumgarten, N. et al. \"Towards Balance and Boundaries in Public Discourse: Expressing and Perceiving Online Hate Speech (XPEROHS)\" (2019), RASK \u2013 International journal of language and communication.","title":"XPEROHS - Expression and Perceiving Online Hate Speech"},{"location":"use_cases/#conspicuous-consumption-in-the-united-states-and-china","text":"Conspicuous consumption describes and explains the consumer practice of buying and using goods of higher quality, price, or greater quantity than practical. This study considered differences in conspicuous consumption in the United States and China. The paper uses a partial-equilibrium, heterogeneous-agent structural model, where the consumer's peer infers the consumer's wealth based on their purchases. The model was simulated and shown to produce data similar to the observed data using HPC. Simulations were also used to estimate welfare gains of sales taxes on various consumer goods. Jinkins, D. \"Conspicuous consumption in the United States and China\" (2016), Journal of Economic Behavior & Organization.","title":"Conspicuous consumption in the United States and China"},{"location":"use_cases/#detection-of-mycobacterium-leprae-in-archeaological-human-dental-calculus-using-hpc-enabled-sequencing","text":"This study is the first to demonstrate the recovery of ancient M. leprae biomolecules from archaeological dental calculus. As dental calculus are pretty robust, it is often conserved over long periods. Therefore, it may represent an alternative sample source to bones and teeth for detecting and molecularly characterizing M. leprae in individuals from the archaeological record, especially in the absence of definitive osteological markers. This is particularly relevant for cases where human remains are poorly preserved or too precious to warrant destructive bone sampling. The M. leprae genome was recovered using shotgun sequencing done using HPC resources. Fotakis, A. K. et al. \"Multi-omic detection of Mycobacterium leprae in archaeological human dental calculus\" (2020), Phil. Trans. R. Soc. B 375: 20190584.","title":"Detection of Mycobacterium leprae in archeaological human dental calculus using HPC-enabled sequencing"},{"location":"use_cases/#hpc-enabled-genomic-sequencing-of-dog-skin-garments","text":"Among Indigenous populations of the Arctic, domestic dogs were social actors aiding in traction and subsistence activities. Less commonly, dogs fulfilled a fur-bearing role in both the North American and Siberian Arctic. This study sequenced the mitochondrial genomes of macroscopically identified dog skin garments. This sequencing was made possible using HPC resources. The study found that dog provisioning practices were variable across the Siberian and North American Arctic, but in all cases, involved considerable human labor. Harris, A. J. T. et al. \"Archives of human-dog relationships: Genetic and stable isotope analysis of Arctic fur clothing\" (2020), Journal of Anthropological Archeaology 59, 101200.","title":"HPC-enabled genomic sequencing of dog skin garments"}]}